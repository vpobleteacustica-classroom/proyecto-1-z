{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "047d525e",
   "metadata": {},
   "source": [
    "# Proyecto de ACUS 220 Acústica Computacional con Python  \n",
    "\n",
    "### Primer Hito de Entrega  \n",
    "\n",
    "**Integrantes:** Martin Maza, Samantha Espinoza, Esperanza Tejeda, Rudy Richter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ce9095",
   "metadata": {},
   "source": [
    "## Objetivos del Proyecto  \n",
    "\n",
    "**Objetivo General:**  \n",
    "- Desarrollar un software capaz de analizar cualquier canción en formato digital y generar automáticamente partituras o tablaturas de guitarra lead, facilitando la práctica y composición para guitarristas de todos los niveles.\n",
    "\n",
    "**Objetivos Específicos:**  \n",
    "- OE1: Implementar un sistema de separación de instrumentos que permita aislar los instrumentos de una canción y obtener sus partituras.\n",
    "\n",
    "- OE2: Desarrollar un algoritmo de reconocimiento de la melodía principal, capaz de identificar las notas que corresponden al instrumento lead y/o a la voz principal de la canción.\n",
    "\n",
    "- OE3: Generar automáticamente las partituras o tablaturas de guitarra lead a partir de la melodía identificada, adaptándolas a un formato legible y reproducible por el usuario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ba83cb",
   "metadata": {},
   "source": [
    "## Estado del Arte \n",
    "\n",
    "En los últimos años, gracias a los avances en la inteligencia artificial y el procesamiento digital de señales, los investigadores han desarrollado diversas técnicas para analizar grabaciones musicales de forma automática. Entre estas destacan los métodos orientados a la identificación de sonidos específicos mediante el análisis de sus frecuencias (Caropresse & Montoya, 2014; Rosner & Kostek, 2018; Vázquez Robledo, Lizárraga Morales, & López Ramírez, 2024), así como los sistemas de transcripción automática de música (Automatic Music Transcription, AMT), cuyo objetivo es transformar una señal de audio en notación musical legible por humanos o interpretable por software (Donis del Álamo, 2021; Chieppa, 2024; Julian & Mukund, 2024). Este tipo de investigación ha cobrado especial fuerza desde el año 2020, impulsada por el creciente acceso a modelos de aprendizaje profundo y a bases de datos musicales de libre disponibilidad (Klangio, s. f.; López-Rincón, Starostenko, & Ayala-San Martín, 2018).\n",
    "\n",
    "En este mismo contexto, han surgido los sistemas de recuperación de información musical (Music Information Retrieval, MIR), un campo de investigación emergente orientado al desarrollo de herramientas de software capaces de extraer, analizar y recuperar información relevante desde archivos de audio (Vázquez Robledo, 2024; Rosner & Kostek, 2018). Estos sistemas combinan técnicas de procesamiento digital de señales, aprendizaje automático (Machine Learning, ML) e inteligencia artificial, lo que permite abordar diversas problemáticas asociadas con la organización y caracterización de la música digital. Entre sus principales aplicaciones se encuentran la identificación de intérpretes o artistas, la detección de canciones, la clasificación por género musical, el análisis de ritmo y tempo, el reconocimiento de emociones musicales y la identificación de instrumentos (Vázquez Robledo, Lizárraga Morales, & López Ramírez, 2024; Rosner & Kostek, 2018). Esta última área, la detección automática de instrumentos musicales, constituye el foco principal del presente proyecto.\n",
    "\n",
    "Para el proyecto propuesto, estas dos técnicas serían importantes ya que permiten descomponer la canción en todos sus elementos musicales (voz, guitarra, bajo, teclado, percusión, etc.) para así utilizar las fuentes sonoras que sean más relevantes, mezclándolas y haciendo una única pieza que combine todos los elementos esenciales de la composición (Donis del Álamo, 2021; Chieppa, 2024). De esta forma, se podrá posteriormente hacer una transcripción de la canción en forma de tablatura para guitarra. La idea es que musicalmente sea atractiva, de modo que la tablatura generada no refleje las notas de la canción original, sino que también mantenga coherencia, dinámica y expresividad, generando una versión que sea agradable de escuchar y fiel a su esencia musical (Julian & Mukund, 2024; López-Rincón, Starostenko, & Ayala-San Martín, 2018)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc17aa5",
   "metadata": {},
   "source": [
    "## Materiales y Métodos  \n",
    "\n",
    "### Materiales  \n",
    "\n",
    "- Audio de entrada: archivos WAV o MP3 (44.1 kHz).\n",
    "- **Herramientas y librerías (Python)**:\n",
    "  - Procesamiento: `librosa`, `numpy`, `scipy`, `soundfile`, `pydub`\n",
    "  - Extracción de melodía/tono: `crepe`, `librosa` (HPSS)\n",
    "  - Manipulación musical y exportación: `music21` (ASCII tab o MusicXML)\n",
    "  - Utilidades: `matplotlib`, `tqdm`, `pandas`\n",
    "- Entorno: Jupyter y Visual Studio Code\n",
    "- Control de versiones: Git y Github\n",
    "\n",
    "\n",
    "### Metodología  \n",
    " \n",
    "**Recolección de datos**\n",
    "- Se analizarán documentos sobre manipulación de audios en python\n",
    "- Se elegiran canciones diversas para probar a la par del avance del proyecto\n",
    "- Utilizar codigos publicos para estudiar el uso de las librerias en python\n",
    "\n",
    "**Cargado y preparación**\n",
    "- Cargar audio (Archivo WAV o mp3), convertir a mono y normalizar, convertir a 44.1 kHz\n",
    "\n",
    "**Separación de instrumentos**\n",
    "- Usando *demucs* separar instrumentos principales, (voz, bajo, guitarra, teclados, bateria)\n",
    "- Descartar instrumentos con SNR muy bajo, osea que no es tan central en la canción\n",
    "\n",
    "**Selección de melodía**\n",
    "- Usando *crepe* para conseguir metricas como la confianza media, suavidad y estabilidad de f0\n",
    "- Elegir como melodía el stem o la separación con mayor influencia, utilizando las metricas\n",
    "\n",
    "**Detectar Melodía**\n",
    "- Usar *crepe* para obtener la curva de la frecuencia fundamental (f0)\n",
    "- Aplicar **HPSS** (Harmonic–Percussive Source Separation) de `librosa` para atenuar componentes percutivos y realzar la parte **armónica** de la mezcla.\n",
    "- Corregir octavas si se detecta algún error\n",
    "\n",
    "**Convertir f0 a notas**\n",
    "- Redondear cada f0 al semitono más cercano usando nota de MIDI\n",
    "- Agrupar frames continuos con la misma nota en segmentos\n",
    "\n",
    "**Cuantizar Tiempos**\n",
    "- Estimar tempo con *librosa* y ajustar inicios/duraciones a notas musicales (negras/corcheas)\n",
    "- Saltarse espacios vacios largos en la canción\n",
    "\n",
    "**Mapear a guitarra y exportar**\n",
    "- Para cada nota (octava), generar candidatos de cuerda y traste válidos según *EADGBE* y rango de trastes.\n",
    "- Hacer función de coste enfocado en la dificultad de la canción en ser tocada, para no crear una tablatura imposible\n",
    "- Exportar tablatura ASCII o MusicMXL\n",
    "\n",
    "**Validaciones**\n",
    "- Verificación manual revisando si la canción es representada correctamente y sea tocable en guitarra, ajustar configuraciones si hay errores.\n",
    "- Revisión automática con metricas como error de pitch, notas fuera del rango de la guitarra, etc.\n",
    "\n",
    "### Plan de trabajo\n",
    "| Actividad                                   | Responsable(s)    | Fecha estimada |\n",
    "|---------------------------------------------|-------------------|----------------|\n",
    "| Revisión bibliográfica básica (melodía/f0)  | Grupo completo    | 10/10/2025     |\n",
    "| Selección de canciones de prueba            | Samantha/Esperanza| 14/10/2025     |\n",
    "| Preparación de audio                        | Rudy              | 20/10/2025     |\n",
    "| Baseline de f0 con CREPE + suavizado        | Samantha          | 27/10/2025     |\n",
    "| f0 → notas (redondeo)                       | Martín            | 5/11/2025      |\n",
    "| Cuantización simple (negras/corcheas)       | Esperanza         | 12/11/2025     |\n",
    "| Mapeo a guitarra                            | Rudy              | 20/11/2025     |\n",
    "| Validaciones                                | Martín / Rudy     | 23/11/2025     |\n",
    "| Exportar tablatura ASCII + revisiones       | Grupo completo    | 25/11/2025     |\n",
    "| Entrega final                               | Grupo completo    | 30/11/2025     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1eb4b2e",
   "metadata": {},
   "source": [
    "## Referencias Bibliográficas  \n",
    "\n",
    "1. Caropresse, G., & Montoya, D. E. A. (2014, noviembre). *Identificación de instrumentos musicales con redes neuronales usando separación de fuentes*. En **Jornadas de Investigación y Encuentro Académico Industrial (JIFI 2014)**, Universidad Central de Venezuela, Caracas, Venezuela. Disponible en ResearchGate: https://www.researchgate.net/publication/275641654_Identificacion_de_Instrumentos_Musicales_con_Redes_Neuronales_Usando_Separacion_de_Fuentes\n",
    "\n",
    "2. Donis del Álamo, J. (2021). *Transcripción musical mediante redes neuronales profundas* (Trabajo de fin de máster). Universidad de Alicante. https://rua.ua.es/bitstream/10045/118065/1/Transcripcion_de_musica_con_redes_neuronales_profundas_Donis_Del_Alamo_Jorge.pdf\n",
    "\n",
    "3. Rosner, A., & Kostek, B. (2018). *Automatic music genre classification based on musical instrument track separation*. **Journal of Intelligent Information Systems, 50**, 363–384. https://doi.org/10.1007/s10844-017-0464-5\n",
    "\n",
    "4. Klangio. (s. f.). *Research*. Recuperado el 6 de octubre de 2025, de https://klang.io/about-us/research\n",
    "\n",
    "5. Vázquez Robledo, A. S., Lizárraga Morales, R. A., & López Ramírez, M. (2024). *Clasificación de instrumentos musicales en audios utilizando coeficientes cepstrales y redes neuronales artificiales*. **Jóvenes en la Ciencia, 33** (Congreso Internacional de Electrónica y Cómputo Aplicado 2024). https://www.jovenesenlaciencia.ugto.mx/index.php/jovenesenlaciencia/article/view/4701\n",
    "\n",
    "6. Vázquez Robledo, A. S. (2024). *Identificación de instrumentos musicales en audios utilizando análisis de señales e inteligencia artificial* (Tesis de maestría, Universidad de Guanajuato). http://repositorio.ugto.mx/bitstream/20.500.12059/13673/3/ALAN_SALOMON_VAZQUEZ_ROBLEDO_TesisMtria24.pdf\n",
    "\n",
    "7. Chieppa, S. (2024). *Automatic guitar transcription using deep neural networks* (Tesis de maestría, Sapienza University of Rome). Publications – MIR/DEI, University of Coimbra. https://mir.dei.uc.pt/publications.html\n",
    "\n",
    "8. López-Rincón, O., Starostenko, O., & Ayala-San Martín, G. (2018). *Algorithmic music composition based on artificial intelligence: A survey*. En 2018 International Conference on Electronics, Communications and Computers (CONIELECOMP) (pp. 187–193). IEEE. https://doi.org/10.1109/CONIELECOMP.2018.8327197\n",
    "\n",
    "9. Julian, A., & Mukund, V. (2024). *Music to score conversion using machine learning*. IEEE. https://ieeexplore.ieee.org/abstract/document/10502423"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
